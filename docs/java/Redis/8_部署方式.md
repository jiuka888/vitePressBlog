Redis 有以下 4 种部署方式：

# 一、单机模式

最基本的部署方式，只需要一台机器负责读写，一般只用于开发人员开发自测。

# 二、主从复制模式

对于缓存来说，一般都是用来支撑**读高并发**的，主从（Master-Slave）模式架构，一主多从，其中  Master 负责写，并且将数据复制到其它的 Slave 节点，而 Slave 节点负责读。所有的**读请求全部访问 Slave节点**，这样也可以很轻松实现水平扩容，支撑读高并发。

单机的 Redis，能够承载的 QPS 大概就在上万到几万不等。对于主从架构，增加 Slave 节点的数量可以使得 Redis的 QPS 达到 10 万以上。

<div align="center"><img src="https://github.com/DuHouAn/ImagePro/raw/master/redis/redis_12.png"/></div>

## 主从同步方法

Master 节点接收到写请求并处理后，需要告知 Slave 节点数据发生变化，保证主从节点数据一致即为主从同步。主从同步方法有：增量同步、快照同步和无盘复制。

### 增量同步

Redis 同步的是指令流，Mater 节点会将对自己的状态产生修改的指令记录在本地 buffer 中，然后异步将 buffer 中的指令同步到 Slave 节点，Slave 节点一边执行同步的指令流来达到和 Master 节点一样的状态，一边向主节点反馈同步偏移量。

Slave 节点同步数据的时候不会影响 Master 节点的正常工作，也不会影响自己对外提供读服务的功能，Slave 节点会用旧的数据来提供服务，当同步完成后，需要删除旧数据集，加载新数据，此时会暂停对外服务。

因为内存的 buffer 是有限的，所以 Master 节点不能将所有的指令都记录在内存 buffer 中。Redis 的复制内存 buffer 是一个定长的环形数组，如果数组内容满了，就会从头开始覆盖前面的内容。

### 快照同步

节点间网络通信不好，当 Slave 节点同步的速度不如 Master 节点接收新写请求的速度时，buffer 中会丢失一部分指令，Slave 节点中的数据将与 Master 节点中的数据不一致，此时将会触发快照同步。

快照同步先在 Master 节点执行 bgsave 生成 rdb 快照文件，然后将快照文件的内容全部传送到 Slave 节点。Slave 节点将快照文件接受完毕后，立即执行一次**全量加载**，加载前会将当前内存数据清空。加载完毕后通知 Master 节点继续进行增量同步。

在整个快照同步进行的过程中，Master 节点的 buffer 仍在往前移动，如果快照同步的时间过长或者 buffer 太小，同步期间的增量指令在 buffer 中被覆盖，这样就会导致快照同步完成后无法进行增量复制，然后会再次发起快照同步，如此极有可能会陷入快照同步的死循环。所以需要配置一个合适的 buffer 大小参数，避免快照复制的死循环。

<div align="center"><img src="https://github.com/DuHouAn/ImagePro/raw/master/redis/redis_13.png"/></div>

### 无盘复制

在进行快照同步时，存在大量的文件 IO 操作，特别是对于非 SSD 磁盘存储时，快照会对系统的负载产生较大影响。特别是当系统正在进行 AOF 的 fsync 操作时如果发生快照复制，fsync 将会被推迟执行，这就会严重影响 Master 节点的服务效率。

从 Redis 2.8.18 版开始支持无盘复制。无盘复制指的是 Master 节点会**一边遍历内存，一遍将序列化的内容发送给 Slave 节点，而不是生成完整的 rdb 快照文件后才进行 IO 传输**。Slave 节点还是跟之前一样，先将接收到的内容存储到磁盘文件中，再进行一次性加载。

## 主从同步完整流程

### 1. 建立连接

- Slave 节点配置文件中的 slaveof 配置项中配置  Master 节点的 IP 和 port 后， Slave 节点就知道自己要和那个 Master 节点进行连接。
- Slave 节点内部有个定时任务，会每秒检查自己要连接的 Master 节点是否上线，如果发现 Master 节点上线，就跟 Master 节点进行网络连接，此时还没有进行主从同步。
- Slave 节点发送 ping 命令给 Master 节点进行连接，如果设置了口令认证（Master 节点设置了requirepass），那么 Slave 节点必须发送正确的口令（masterauth）进行认证。

### 2. 快照同步

- Master 和 Slave 节点连接成功后，判断是否进行快照同步：如果 Slave 节点发现已经连接过某个 `run id` 的 Master 节点，则此次连接为重新连接，不会进行快照同步。相同 IP 和 port 的 Master 节点每次重启服务都会生成一个新的 `run id`，所以每次 Master 节点重启服务都会进行一次快照同步，如果想重启主节点服务而不改变 `run id`，使用 `redis-cli debug reload` 命令。

- 进行快照同步开始后， Master 节点在本地生成一份 rdb 快照文件，并将这个 rdb 文件发送给 Slave 节点，如果复制时间超过 60 秒（配置项：`repl-timeout`），那么就会认为复制失败，所以数据量比较大，要适当调大这个参数的值。

  进行快照同步时，Master 节点会把接收到的新请求命令写到 buffer 中，当快照同步完成后，再把 buffer 中的指令增量同步到 Slave 节点。如果在快照同步期间，内存缓冲区大小超过 64MB / 256MB 的状态持续时间超过60s（配置项：`client-output-buffer-limit slave 256MB 64MB 60`），那么快照同步失败。

- Slave 节点接收到 rdb 快照文件后，清空自己的旧数据，然后重新加载 rdb 到自己的内存中，在这个过程中基于旧的数据对外提供服务。如果 Master 节点开启了AOF，那么在快照同步结束后会立即执行 BGREWRITEAOF 命令，重写 AOF 文件。

- Master 节点维护了一个 backlog 文件，默认大小是 1MB 。Master 节点向 Slave 节点发送全量数据（即 rdb 快照文件）时，也会同步到 backlog 中，这样当发送全量数据这个过程意外中断后，从 backlog 文件中可以得知数据有哪些是发送成功了，哪些还没有发送，然后当主从节点再次连接后，从失败的地方开始增量同步。**当快照同步连接中断后，主从节点再次连接并非是第一次连接，进行增量同步，而不是继续进行快照同步**。

### 3. 增量同步

- 快照同步完成后，Master 节点后续接收到写请求导致数据变化后，将和 Slave 节点进行增量同步，遇到 buffer 溢出则会再触发快照同步。

## 主从同步补充

- 主从节点都会维护一个 offset，随着 Master 节点的数据变化以及主从同步的进行，主从节点会不断累加自己维护的 offset，Slave 节点每秒都会上报自己的 offset 给 Master 节点，Master 节点也会保存每个  Slave 节点的 offset，这样主从节点就能知道互相之间的数据一致性情况。

  Slave 节点发送`psync runid offset`命令给 Master 节点从而开始主从同步，Master 节点会根据自身的情况返回响应信息，可能是`FULLRESYNC runid offset`触发快照复制，也可能是 `CONTINUE` 触发增量复制。

- 主从节点因为网络原因导致断开，当网络接通后，不需要手工干预，可以自动重新连接。

- Master 节点如果发现有多个 Slave 节点连接，在快照同步过程中仅仅会生成一个 red 快照文件，用一份数据服务所有 Slave 节点进行快照同步。

- Slave 节点不会处理过期 key，当 Slave 节点处理了一个过期 key，会模拟一条 del 命令发送给 Slave 节点。

- 主从节点会保持心跳（heartbeat）来检测对方是否在线，Master 节点默认每隔 10 秒发送一次 heartbeat，Slave 节点默认每隔 1 秒发送一个heartbeat。

- 建议 Master 节点使用 AOF+RDB 的持久化方式，并且在 Master 节点定期备份 red 快照文件，而 Slave 节点不要开启 AOF 机制，原因有两个：

  - Slave 节点开启 AOF 会降低性能
  - 如果 Master 节点数据丢失，数据同步给 Slave 节点后，Slave 节点会收到空数据，如果开启了 AOF，会生成空的 AOF 文件，基于 AOF 恢复数据后，全部数据就都丢失了，而如果不开启 AOF 机制， Slave 节点启动后，基于自身的 rdb 快照文件恢复数据，不至于丢失全部数据。

扩展：[主从模式实战](https://www.jianshu.com/p/f0e042b95249)



# 三、哨兵模式

在 Redis 主从复制模式下，一旦 Master 节点由于故障不能提供服务，需要**人工**将 Slave 节点晋升为  Master 节点，再通知所有的程序把 Master 地址统统改一遍，然后重新上线。

Redis 从 2.8 开始正式提供了 **哨兵 (sentinel) 架构**来解决这个问题。

Redis 中多个 sentinel 组成**分布式架构**，持续监控主从节点的健康状况，当 Master 节点挂掉时，自动选择一个最优的 Slave 节点切换为 Master 节点。客户端在连接集群时，会先连接 sentinel，通过 sentinel 来查询 Master 节点的地址，然后再去连接 Master 节点进行数据交互。当 Master 节点发生故障时，客户端会重新向 sentinel 获取地址，sentinel 会将最新的 Master 节点地址告诉客户端，这样应用程序将无需重启即可自动完成节点切换。

<div align="center"><img src="https://github.com/DuHouAn/ImagePro/raw/master/redis/redis_15.png"/></div>

sentinel 集群在 Redis 主从架构高可用中起到的 4 个作用：

- 集群监控
  sentinel 节点会定期检测 Redis 数据节点、其余 sentinel 节点是否故障。
- 故障转移
  实现 Slave 节点晋升为 Master 节点并维护后续正确的主从关系。
- 配置中心
  sentinel 架构中，客户端在初始化的时候连接的是 sentinel 集群，从中获取 Master 节点信息。
- 消息通知
  sentinel 节点会将故障转移的结果通知给客户端。

注意：使用 sentinel 集群而不是单个 sentinel 节点去监控 Redis 主从架构，这是因为：

- 对于节点的故障判断由多个 sentinel 节点共同完成，这样可**有效防止误判**。
- sentinel 集群可以保证自身的**高可用性**，即某个 sentinel 节点自身故障也不会影响 sentinel 集群的健壮性。

## 监控功能

sentinel 集群通过 3 个定时监控任务完成对各个节点发现和监控：

- 每隔10 秒，每个 sentinel 节点会向 Master 节点和 Slave 节点发送 info 命令获取 Redis 主从架构的最新情况。
- 每隔 2 秒，每个 sentinel 节点会向 Redis 数据节点的 `__sentinel__:hello` 这个channel（频道）发送一条消息，该定时任务可以完成以下 2 个工作：
  - 发现新的 sentinel节点
  - sentinel 节点之间交换 Master 节点的状态，用于确认 Master 下线和故障处理的 Leader 选举。
- 每隔 1 秒，每个 sentinel 节点会向 Master 节点、Slave 节点、其余 sentinel 节点发送一条 ping 命令做一次心跳检测，来确认这些节点是否可达。

## sdown & odown

sdown 即主观下线 (subjective down)  ：一个 sentinel 节点每隔 1 秒对 Master 节点、Slave 节点、其他 sentinel 节点发送 ping 命令做心跳检测，当某个节点超过
 `down-after-milliseconds` 没有进行有效回复，该 sentinel 节点就会认为该节点下线，这就是主观下线。

odown 即客观下线 (objective down) ：超过 quorum 数量（quorum可配置）的 sentinel 节点认为 Master 节点确实有问题，即大部分是 sentinel 节点都对 Master 节点的下线做了同意的判定，这就是客观下线。

## quorum & majority

只有大于等于 quorum 数量的 sentinel 节点都认为 Master 主观下线，sentinel 集群才会认为 Master 客观下线。

quorum 可在 sentinel.conf 中手动配置，默认值为 2：

```
# sentinel monitor [master-name] [master-ip] [master-port] [quorum] 
sentinel monitor testmaster 127.0.0.1 6379 2
# quorum 默认值是 2 
```

majority 代表 sentinel 集群中大部分 sentinel 节点数，majority 的计算方法为：majority = num(sentinels) / 2 + 1。例如 2 个节点的 sentinel 集群的 majority 为 2
 3 个节点的 sentinel 集群的 majority 为 2；4 个节点的 sentinel 集群的 majority为 3。

只有大于等于 max(quorum, majority)  个节点给某个 sentinel 节点投票，该 sentinel 节点才能被选为 Leader。

注意：sentinel 集群的节点个数至少为 3 个。

原因如下：假设当节点数为 2 时，此时一个 sentinel 节点宕机，那么剩余一个节点是无法让自己成为 Leader 的，因为 2 个节点的 sentinel 集群的 majority 是 2，此时没有 2 个节点都给剩余的节点投票，也就无法选择出 Leader，从而无法进行故障转移。

此外最好把 quorum 的值设置为小于等于 majority，否则即使 sentinel 集群剩余的节点满足 majority 数，但是有可能不能满足 quorum 数，那还是无法选举 Leader，也就不能进行故障转移。

## Leader 选举

当 sentinel 集群确认某个 Master 是客观下线了，需要选举出一个 **Leader 节点来进行故障转移**，选举过程如下：

- 每个在线的 sentinel 节点都有资格成为 Leader，当某个 sentinel 节点确认 Master 节点 odown 时，会向其他 sentinel 节点发送 `sentinel is-master-down-by-addr`命令，要求将自己设置为 Leader。
- 每个 sentinel 节点都只能投出一票，如果该 sentinel 节点得到的**票数大于 max(quorum, num(sentinels) / 2 + 1)**  ，那么该 sentinel 节点成为 Leader。
- 如果一次选举没有选举出 Leader，那么会进行下一次选举。

一般情况下，哪个 sentinel 节点最先确认 Master 客观下线，该 sentinel 节点就会成为执行故障转移的 Leader。

## 新 Master 选择

要执行故障转移，首先要从 Slave 中选择一个作为新的 Master。

不选择不健康的 Slave，以下状态的 slave 是不健康的：

- 主观下线的 Slave
- 与 Master 失联超过`down-after-milliseconds * 10`秒的 Slave
- 大于等于 5 秒没有回复过 sentinel 节点 ping 响应的 Slave

对健康的 slave 进行排序：

- 选择 priority（可配置，默认100）最低的 Slave 节点，如果有优先级相同的节点，进行下一步。注意如果  priority=0，那么禁止该节点成为 Master
- 选择复制偏移量最大的 Slave 节点（复制的最完整），如果有复制偏移量相等的节点，进行下一步
- 选择 runid 最小的 Slave 节点

## 故障转移

选举出 Leader 和新 Master 后，由 Leader 进行故障转移：

- Leader 对选举出的新 Master（此时还是 Slave）执行 `slaveof no one` 命令让其成为 Master。
- Leader 向剩余 Slave 发送命令，使它们称为新 Master 的 Slave。
- Leader 会将原来的 Master 更新为 Slave，并保持着对其关注，当其恢复后命令它去复制新 Master。

## configuration epoch

configuration epoch 是当前 Redis 主从架构的配置版本号，无论是 sentinel 集群选举 Leader 还是进行故障转移的时候，要求各 sentinel 节点得到的 configuration epoch 都是相同的，`sentinel is-master-down-by-addr` 命令中就必须有当前配置版本号这个参数，在选举 Leader 过程中，如果本次选举失败，那么进行下一次选举，就会更新配置版本号，也就是说，每次选举都对应一个新的 configuration epoch，在故障转移的过程中，也要求各个 sentinel 节点使用相同的 configuration epoch。

在故障转移成功之后，Leader 会更新生成最新的 master 配置，configuration epoch 也会更新，然后同步给其他的 sentinel 节点，这样保证 sentinel 集群中保存的 Master 和 Slave 配置都是最新的，当 Client 请求的时候就会拿到最新的配置信息。

## 数据丢失问题

### 数据丢失原因

Redis sentinel 可能会导致数据丢失：

- 异步复制导致的数据丢失

  Master 到 Slave 的复制是异步的，所以可能有部分数据还没复制到 Slave，Master 就宕机了，此时这部分数据就丢失了。

- 脑裂导致的数据丢失

  某个 Master 所在机器突然网络故障，跟其他 Slave 机器不能连接，但是实际上 Master 还运行着。sentinel  会认为该 Master 宕机了，然后开启选举，将其他 Slave 切换成了 Master，集群里就会有两个 Master，也就是所谓的脑裂。

  虽然某个 Slave 被切换成了 Master，但是 Client 还没来得及切换到新的 Master，还继续写向旧 Master 。旧 Master 再次恢复的时候，会被作为一个 Slave 挂到新的 Master 上去，自己的数据会清空，重新从新的 Master 复制数据，而新的 Master 并没有后来 Client 写入的数据，这部分数据就丢失了。

### 解决方案

Redis 提供了两个配置参数可以尽量丢失少的数据：

```
min-slaves-to-write 1
min-slaves-max-lag 10
```

Master 至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒。如果说一旦所有的 Slave，数据复制和同步的延迟都超过了 10 秒钟，那么 Master 就不会再接收任何请求。

- 减少异步复制数据的丢失

  `min-slaves-max-lag` 配置可确保一旦 Slave 复制数据和 ack 延时太长，就认为可能 Master 宕机后损失的数据太多了，那么就拒绝写请求，这样可以把 Master 宕机时由于部分数据未同步到 Slave 导致的数据丢失降低的可控范围内。

- 减少脑裂的数据丢失

  如果一个 Master 出现了脑裂，跟其他 Slave 丢了连接，那么上面两个配置可以确保如果不能继续给指定数量的 Slave 发送数据且 Slave 超过 10 秒没有给自己 ack 消息，那么就直接拒绝客户端的写请求。因此在脑裂场景下，最多就丢失 10 秒的数据。

注意：redis-2.6 版本提供的是 redis sentinel v1 版本，但是功能性和健壮性都有一些问题。如果想使用 redis sentinel 的话，建议使用 2.8 以上版本。

扩展：[哨兵模式实战](https://www.jianshu.com/p/9d873e7a205a)



# 四、Cluster 集群模式

## Redis Cluster 架构

单 Master 架构中 Master 节点的数据和 Slave 节点的数据是一模一样的，Master 节点的最多能容纳多少数据量，Slave 节点也就只能容纳这么多数据。当数据量超过 Master 的内存，Redis 会使用 LRU 算法清除部分数据。如果确实要容纳更多的数据量，Redis 主从架构是无法解决这个问题的。

Redis Cluster 是 Redis 的**分布式解决方案**，Redis 3.0 正式推出，解决单Master 架构的内存、并发、流量等瓶颈，以达到负载均衡的目的。

<div align="center"><img src="https://github.com/DuHouAn/ImagePro/raw/master/redis/redis_14.png"/></div>

Redis cluster 适用于海量数据、高并发、高可用场景，在 Redis Cluster 架构中：

- 每个 Master 负责整个集群的一部分数据，每个节点负责数据量可能不一样
- 每个 Master 的角色是对等的，每个 Master 节点都可以有多个 Slave 节点，当一个 Master 节点挂掉后，它的其中一个 Slave 节点升级为 Master
- Redis Cluster 已经自动具备了主从复制能力，直接集成了 replication 和 sentinel 的功能。

## 分布式寻址算法

### Hash 算法

#### 思路

根据 key 计算 Hash 值，然后对节点数取模，将 key 分配到对应的 Master 中。

#### 问题

一旦某个 Master 宕机，就需要调整 hash 算法，所有的数据都需要重新计算取模，然后重新分配数据。大部分的请求都无法正确的拿到数据，从而不得不去访问数据库，在高并发场景下，这样是不能接受的，所以目前分布式缓存不再使用此种算法分配数据。

### 一致性 Hash 算法

#### 思路

将哈希空间 [0, 2^n-1] 看成一个哈希环，每个服务器节点都配置到哈希环上。每个数据对象通过哈希取模得到哈希值之后，存放到哈希环中**顺时针方向**第一个大于等于该哈希值的节点上。

例如有 Object A、Object B、Object C、Object D 四个数据对象，经过哈希计算后，在哈希环上的位置如下：

<div align="center"><img src="https://github.com/DuHouAn/ImagePro/raw/master/redis/r_13.jpg" width='500px'/></div>

根据一致性 Hash 算法：Object A 会被定位到 Node A 上；Object B 会被定位到 Node B 上；Object C 会被定位到 Node C 上；Object D 会被定位到 Node D 上。

一致性哈希在增加或者删除节点时只会影响到哈希环中相邻的节点：

例如删除节点 C，只需将 C 上的数据分布到节点 D 即可。

<div align="center"><img src="https://github.com/DuHouAn/ImagePro/raw/master/redis/r_14.jpg" width='500px'/></div>

例如新增节点 X，只需要将它前一个节点 C 上的数据重新进行分布即可，对于节点 A、B、D 都没有影响。

<div align="center"><img src="https://github.com/DuHouAn/ImagePro/raw/master/redis/r_15.jpg" width='500px'/></div>

综上所述，一致性 Hash 算法对于节点的增减都只需**重定位哈希环中的一小部分数据**，具有**较好的容错性和可扩展性**。

#### 问题

- 加减节点会造成哈希环中部分数据无法命中，当一个 Master 挂掉之后，例如节点 A 挂掉，那么当请求从 A 获取数据时，是获取不到的，于是继续顺时针去节点 B、节点 C、节点 D 获取数据，当然也是获取不到的，需要手动处理这些无法命中的数据
- 节点数量越多，增减节点带来的影响越小，因此不适用与集群中只有少量节点的情况
- 容易造成数据热点问题

#### 优化-虚拟节点

一致性哈希存在数据分布不均匀的问题，节点存储的数据量有可能会存在很大的不同。数据不均匀主要是因为节点在哈希环上分布的不均匀，这种情况在节点数量很少的情况下尤其明显。

<div align="center"><img src="https://github.com/DuHouAn/ImagePro/raw/master/redis/r_16.jpg" width='325px'/></div>

解决方式是通过增加虚拟节点，然后将虚拟节点映射到真实节点上。虚拟节点的数量比真实节点来得多，那么虚拟节点在哈希环上分布的均匀性就会比原来的真实节点好，从而使得数据分布也更加均匀。

例如针对上面的情况，可以为每台服务器计算 3 个虚拟节点：

- Node A 的 3 个虚拟节点："Node A#1"、"Node A#2"、"Node A#3"
- Node B 的 3 个虚拟节点："Node B#1"、"Node B#2"、"Node B#3"

<div align="center"><img src="https://github.com/DuHouAn/ImagePro/raw/master/redis/r_17.jpg" width='500px'/></div>

同时**数据定位算法不变**，只是多了一步虚拟节点到实际节点的映射过程，例如"Node A#1"、"Node A#2"、"Node A#3" 这 3 个虚拟节点的数据均定位到 Node A 上，解决了服务节点少时数据倾斜的问题。在实际应用中，通常将虚拟节点数设置为 32 甚至更大，因此即使很少的服务节点也能做到相对均匀的数据分布。

### hash slot 算法

#### 思路

Redis Cluster **使用 16384 个槽 (slot)** 来管理一段整数集合 (哈希值)，slot 是集群内数据管理和迁移的基本单位。

每个 Master 节点负责管理一部分 slot，例如有 5 个节点，那个每个节点管理大约 3276 (16384 / 5 )个槽。对每个 key 使用 CRC32 算法进行哈希得到哈希值，使用哈希值对 16384 进行取模，得到此数据应该分配到的 slot 编号。每个 Master 管理的 slot 的信息就缓存在本地，客户端连接集群时，会获取集群 slot 配置信息，从而通过 key 精确找到 slot 所在的节点，此外还可以强制分配某个 key 到指定的 slot 上。

#### 优点

- 解耦数据和节点之间的关系，简化了节点扩容和收缩难度：

  增加一个 Master，就让其他的 Master 分一部分 slot 给新的 Master 管理；

  移除一个 Master，就把这个 Master 管理的 slot 分配给其他的 Master。

- 某个 Master 挂掉，不影响整个集群，因为请求是到 slot，而不是到 Master，但在 slot 迁移完成之前，请求到挂掉的节点也是不行的。

- slot 迁移的过程是很快的

- 节点自身维护 slot 的映射关系，无需人为管理

- 支持槽、节点、key之间的映射关系查询

### Redis 故障恢复

判断故障的逻辑其实与哨兵模式有点类似，在集群中，每个节点都会**定期的向其他节点发送 ping 命令**，通过有没有收到回复来判断其他节点是否已经下线。

如果**长时间没有回复，那么发起 ping 命令的节点就会认为目标节点疑似下线**，也可以和哨兵一样称作主观下线，当集群中超过半数的节点都认为该节点下线则判定该节点下线。

对宕机的 Master 节点，从其所有的 Slave 节点中选择一个切换成 Master，需要检查每个 Slave 节点与 Master 节点断开连接的时间，如果超过了 `cluster-node-timeout * cluster-slave-validity-factor` ，那么该 Slave 就没有资格切换成  Master。

每个 Slave 节点，都根据自己对 Master 复制数据的 offset，来设置一个选举时间，offset 越大（复制数据越多）的 Slave 节点，选举时间越靠前，优先进行选举。

所有的 Master 节点给要进行选举的 Slave 进行投票，如果超过半数的 Master 节点都投票给了某个 Slave 节点，那么选举通过，那个该  Slave 节点可以切换成 Master。

扩展：[Redis Cluster搭建及测试](https://www.jianshu.com/p/48713d6f35b8)

补充：[Redis-简书](https://www.jianshu.com/nb/33672761)
